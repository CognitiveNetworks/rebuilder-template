# SRE Agent Runtime — Environment Variables
#
# Copy this file to .env and fill in the values for local development.
# NEVER commit .env files with real values.
#
# In production, these are injected from your cloud provider's secrets manager.

# Required — LLM provider (OpenAI-compatible: GitHub Models, OpenAI, Azure, etc.)
# Default base URL points to GitHub Models. Use your GitHub PAT as the key.
# NOT NEEDED for Vertex AI — uses Application Default Credentials instead.
LLM_API_KEY=ghp_your-github-personal-access-token

# Required — PagerDuty
PAGERDUTY_API_TOKEN=your-pagerduty-api-token

# Required — Auth token for /ops/* endpoints on monitored services
OPS_AUTH_TOKEN=your-ops-auth-token

# Required — Comma-separated service registry: name|url|critical
SERVICE_REGISTRY=api|http://localhost:8000|true,worker|http://localhost:8001|false

# Optional — LLM model (default: gpt-4o)
# LLM_MODEL=gpt-4o

# Optional — LLM API base URL (default: https://models.inference.ai.azure.com)
# For OpenAI:    https://api.openai.com/v1
# For Azure:     https://your-resource.openai.azure.com/openai/deployments/your-deployment
# For Vertex AI: https://REGION-aiplatform.googleapis.com/v1beta1/projects/PROJECT/locations/REGION/endpoints/openapi
# LLM_API_BASE_URL=https://models.inference.ai.azure.com

# --- Vertex AI (recommended for GCP projects) ---
# Uncomment these and comment out LLM_API_KEY above to use Vertex AI.
# Auth is via ADC: gcloud auth application-default login
# LLM_MODEL=google/gemini-2.0-flash
# LLM_API_BASE_URL=https://us-central1-aiplatform.googleapis.com/v1beta1/projects/YOUR_PROJECT/locations/us-central1/endpoints/openapi

# Optional — Two-phase model escalation
# Start with the fast model (LLM_MODEL) for initial triage. If the agent
# hasn't resolved the issue by turn N, switch to the escalation model for
# deeper reasoning. Set LLM_MODEL_ESCALATION to enable.
# LLM_MODEL_ESCALATION=google/gemini-2.5-pro
# LLM_ESCALATION_TURN=5

# Optional — Path to WINDSURF_SRE.md (default: /app/WINDSURF_SRE.md)
SRE_PROMPT_PATH=../WINDSURF_SRE.md

# Optional — Incident report output directory (default: /app/incidents)
INCIDENTS_DIR=./incidents

# Optional — PagerDuty Events API v2 routing key (required for GCP Direct mode)
# The agent uses this to CREATE PagerDuty incidents when it cannot resolve an alert.
# Get this from PagerDuty → Service → Integrations → Events API v2 → Integration Key
# PAGERDUTY_ROUTING_KEY=your-events-api-v2-integration-key

# Optional — PagerDuty webhook signing secret for HMAC signature verification (Mode 2 only)
# PAGERDUTY_WEBHOOK_SECRET=your-webhook-signing-secret

# Optional — PagerDuty escalation policy ID for human handoff
# PAGERDUTY_ESCALATION_POLICY_ID=PABCDEF

# Optional — Per-service scaling limits: name|min|max|mode (comma-separated)
# Modes: 'application' (calls POST /ops/scale), 'cloud_native' (cloud provider API)
# SCALING_LIMITS=api|2|10|application,worker|1|5|cloud_native

# Optional — Alert intake pipeline
# MAX_CONCURRENT_ALERTS=3
# ALERT_QUEUE_TTL_SECONDS=600

# Optional — Token budget controls
# Per-incident limit: agent escalates to human if exceeded (default: 100000, 0 = unlimited)
# MAX_TOKENS_PER_INCIDENT=100000
# Hourly rolling limit: agent switches to escalate-only mode when exceeded (default: 0 = unlimited)
# MAX_TOKENS_PER_HOUR=0

# OpenTelemetry — set OTEL_EXPORTER_OTLP_ENDPOINT to enable telemetry export
# When not set, OTEL runs as a no-op — the service works identically without it.
# OTEL_SERVICE_NAME=sre-agent
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf
# OTEL_RESOURCE_ATTRIBUTES=deployment.environment=dev
